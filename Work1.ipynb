{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install numpy\n",
    "# %pip install -q numpy scikit-learn imbalanced-learn\n",
    "# %pip install ucimlrepo matplotlib\n",
    "# %pip install -U imbalanced-learn fcmeans\n",
    "# %pip install -U scikit-learn\n",
    "# %pip install seaborn\n",
    "# %pip install nbformat\n",
    "# %pip show nbformat\n",
    "# %pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import necessary libraries\n",
    "# from sklearn.datasets import make_classification\n",
    "# import pandas as pd\n",
    "# from umap import UMAP\n",
    "# import plotly.express as px\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Generate a synthetic dataset in a single go\n",
    "# X, y = make_classification(\n",
    "#     n_samples=20,   # Total number of samples\n",
    "#     n_features=2,    # Total number of features\n",
    "#     n_informative=2,  # Number of informative features\n",
    "#     n_redundant=0,   # Number of redundant features\n",
    "#     n_classes=2,weights=[0.3,0.7],      # Number of classes\n",
    "#     random_state=42,\n",
    "#     n_clusters_per_class=1\n",
    "# )\n",
    "\n",
    "# # Combine features and target into a DataFrame\n",
    "# df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(X.shape[1])])  # Feature DataFrame\n",
    "# df['species'] = y  # Add target column (species)\n",
    "\n",
    "# # Replace species labels with integers\n",
    "# unique_values = pd.unique(y)  # Use pd.unique directly on the array\n",
    "# for i in range(len(unique_values)):\n",
    "#     df['species'] = df['species'].replace(unique_values[i], i)\n",
    "\n",
    "# # Display the first few rows of the DataFrame\n",
    "# print(\"First few rows of the dataset:\")\n",
    "# print(df.head())\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # Get detailed information about the DataFrame\n",
    "# print(\"Detailed information about the DataFrame:\")\n",
    "# print(df.info())\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # Get summary statistics\n",
    "# print(\"Summary statistics of the dataset:\")\n",
    "# print(df.describe())\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # Check for missing values\n",
    "# print(\"Missing values in the dataset:\")\n",
    "# print(df.isnull().sum())\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # Get the count of different species\n",
    "# species_counts = df['species'].value_counts()\n",
    "# print(\"Counts of different species in the dataset:\")\n",
    "# print(species_counts)\n",
    "# print(\"\\n\")\n",
    "\n",
    "# # Separate features for UMAP projection (excluding 'species' column)\n",
    "# features = df.iloc[:, :-1]\n",
    "\n",
    "# # UMAP for 2D and 3D projections\n",
    "# umap_2d = UMAP(n_components=2, init='random', random_state=42)\n",
    "# umap_3d = UMAP(n_components=3, init='random', random_state=42)\n",
    "\n",
    "# # Perform UMAP projections\n",
    "# proj_2d = umap_2d.fit_transform(features)  # 2D projection\n",
    "# proj_3d = umap_3d.fit_transform(features)  # 3D projection\n",
    "\n",
    "# # 2D Scatter plot using Plotly\n",
    "# fig_2d = px.scatter(\n",
    "#     proj_2d, x=0, y=1,\n",
    "#     color=df['species'], labels={'color': 'species'},\n",
    "#     title='2D UMAP Projection'\n",
    "# )\n",
    "\n",
    "# # 3D Scatter plot using Plotly\n",
    "# fig_3d = px.scatter_3d(\n",
    "#     proj_3d, x=0, y=1, z=2,\n",
    "#     color=df['species'], labels={'color': 'species'},\n",
    "#     title='3D UMAP Projection'\n",
    "# )\n",
    "# fig_3d.update_traces(marker_size=5)\n",
    "\n",
    "# # Display the plots\n",
    "# fig_2d.show()\n",
    "# fig_3d.show()\n",
    "\n",
    "# # Pairplot to visualize relationships (optional)\n",
    "# sns.pairplot(df, hue='species')\n",
    "# plt.title('Pairplot of Dataset')\n",
    "# plt.show()\n",
    "\n",
    "# X= df.drop('species', axis=1)\n",
    "# y= df['species']\n",
    "# print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import pandas as pd\n",
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fetch dataset from UCI\n",
    "dataset = fetch_ucirepo(id=109)  # Fetch dataset with id 53 (replace if needed)\n",
    "\n",
    "# Data as pandas DataFrames\n",
    "X = dataset.data.features  # Feature data\n",
    "y = dataset.data.targets    # Target data\n",
    "\n",
    "# Combine features and target into a single DataFrame\n",
    "df = pd.DataFrame(X, columns=dataset.data.feature_names)  # Create DataFrame for features\n",
    "df['species'] = y  # Add the target column (species)\n",
    "\n",
    "unique_values = pd.unique(y.iloc[:, 0])\n",
    "for i in range(len(unique_values)):\n",
    "    df['species'] = df['species'].replace(unique_values[i], i)\n",
    "\n",
    "X= df.drop('species', axis=1)\n",
    "y= df['species']\n",
    "\n",
    "\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(df.head())\n",
    "print(df.tail())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get detailed information about the DataFrame\n",
    "print(\"Detailed information about the DataFrame:\")\n",
    "print(df.info())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get summary statistics\n",
    "print(\"Summary statistics of the dataset:\")\n",
    "print(df.describe())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in the dataset:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get the count of different species\n",
    "species_counts = df['species'].value_counts()\n",
    "print(\"Counts of different species in the dataset:\")\n",
    "print(species_counts)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Separate features for UMAP projection (excluding 'species' column)\n",
    "features = df.iloc[:, :-1]\n",
    "\n",
    "# UMAP for 2D and 3D projections\n",
    "umap_2d = UMAP(n_components=2, init='random', random_state=0)\n",
    "umap_3d = UMAP(n_components=3, init='random', random_state=0)\n",
    "\n",
    "# Perform UMAP projections\n",
    "proj_2d = umap_2d.fit_transform(features)  # 2D projection\n",
    "proj_3d = umap_3d.fit_transform(features)  # 3D projection\n",
    "\n",
    "# 2D Scatter plot using Plotly\n",
    "fig_2d = px.scatter(\n",
    "    proj_2d, x=0, y=1,\n",
    "    color=df['species'], labels={'color': 'species'}\n",
    ")\n",
    "\n",
    "# 3D Scatter plot using Plotly\n",
    "fig_3d = px.scatter_3d(\n",
    "    proj_3d, x=0, y=1, z=2,\n",
    "    color=df['species'], labels={'color': 'species'}\n",
    ")\n",
    "fig_3d.update_traces(marker_size=5)\n",
    "\n",
    "# Display the plots\n",
    "fig_2d.show()\n",
    "fig_3d.show()\n",
    "\n",
    "# Pairplot to visualize relationships (optional)\n",
    "# sns.pairplot(df, hue='species')\n",
    "# plt.title('Pairplot of Dataset')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "neighbors = NearestNeighbors(n_neighbors=20)\n",
    "neighbors_fit = neighbors.fit(X)\n",
    "distances, indices = neighbors_fit.kneighbors(X)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.plot(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from umap import UMAP\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def extract_k_clusters_with_dbscan_and_visualize(X, k, eps=0.5, min_samples=5):\n",
    "    \"\"\"\n",
    "    Extracts the first k clusters from the data using the DBSCAN algorithm, \n",
    "    reduces dimensions using UMAP, and visualizes the clusters, including noise points.\n",
    "    Also calculates and prints the silhouette score for the clustering.\n",
    "\n",
    "    Args:\n",
    "    - X: Feature matrix\n",
    "    - k: Number of clusters to extract\n",
    "    - eps: Maximum distance between two samples for them to be considered as in the same neighborhood.\n",
    "    - min_samples: Minimum number of samples in a neighborhood for a point to be considered as a core point.\n",
    "\n",
    "    Returns:\n",
    "    - k_clusters: List of the first k clusters (without reduced dimensions).\n",
    "    \"\"\"\n",
    "    # Step 1: Apply DBSCAN to get the clusters\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    dbscan.fit(X)\n",
    "\n",
    "    # Get the cluster labels\n",
    "    cluster_labels = dbscan.labels_\n",
    "\n",
    "    # Step 2: Identify the first k clusters\n",
    "    unique_labels = np.unique(cluster_labels)\n",
    "    k_clusters = []\n",
    "    selected_clusters = []\n",
    "    selected_labels = []\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        if label != -1 and len(k_clusters) < k:  # Ignore noise (-1) and get k clusters\n",
    "            cluster = X[cluster_labels == label]\n",
    "            k_clusters.append(cluster)\n",
    "            selected_clusters.append(cluster)\n",
    "            selected_labels.append(np.full(len(cluster), label))\n",
    "    \n",
    "    # Step 3: Add noise to selected data for visualization\n",
    "    noise = X[cluster_labels == -1]  # Extract noise points\n",
    "    if len(noise) > 0:\n",
    "        selected_clusters.append(noise)\n",
    "        selected_labels.append(np.full(len(noise), -1))  # Mark noise points as -1\n",
    "\n",
    "    # Concatenate all selected clusters into one matrix for UMAP\n",
    "    if len(selected_clusters) > 0:\n",
    "        X_selected = np.vstack(selected_clusters)\n",
    "        labels_selected = np.concatenate(selected_labels)\n",
    "    else:\n",
    "        X_selected = np.array([])  # Empty case\n",
    "        labels_selected = np.array([])\n",
    "    # Step 4: Reduce dimensions using UMAP\n",
    "    umap_2d = UMAP(n_components=2, random_state=42)\n",
    "    reduced_X = umap_2d.fit_transform(X_selected)\n",
    "\n",
    "\n",
    "\n",
    "    # Step 5: Visualize the reduced clusters, including noise\n",
    "    plt.figure(figsize=(10, 7))\n",
    "\n",
    "    # Generate a color palette for clusters (excluding noise, which is black)\n",
    "    palette = sns.color_palette(\"hsv\", len(np.unique(labels_selected)) - 1)  # Exclude noise from palette\n",
    "    \n",
    "    for label in np.unique(labels_selected):\n",
    "        if label == -1:\n",
    "            # Plot noise points in black\n",
    "            plt.scatter(reduced_X[labels_selected == label, 0], reduced_X[labels_selected == label, 1], \n",
    "                        c='black', label='Noise', s=15, alpha=0.6)\n",
    "        else:\n",
    "            # Plot clusters with different colors\n",
    "            plt.scatter(reduced_X[labels_selected == label, 0], reduced_X[labels_selected == label, 1], \n",
    "                        c=[palette[label] if label < len(palette) else 'gray'], \n",
    "                        label=f'Cluster {label}', s=15, alpha=0.7)\n",
    "    \n",
    "\n",
    "    non_noise_indices = cluster_labels != -1  # Exclude noise (-1) points\n",
    "    if np.unique(cluster_labels[non_noise_indices]).size > 1:\n",
    "        silhouette_avg = silhouette_score(X[non_noise_indices], cluster_labels[non_noise_indices])\n",
    "        print(f'Silhouette Score for the clustering: {silhouette_avg:.4f}')\n",
    "    else:\n",
    "        print('Silhouette score could not be calculated because only one cluster was found.')\n",
    "    # Set title and axis labels\n",
    "    plt.title(f'UMAP Projection of First {k} DBSCAN Clusters (with Noise)')\n",
    "    plt.xlabel('UMAP Dimension 1')\n",
    "    plt.ylabel('UMAP Dimension 2')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "   \n",
    "\n",
    "    return k_clusters  # Only return the extracted clusters\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "print(\"Extracting the clusters using DBSCAN...\")\n",
    "Clusters = extract_k_clusters_with_dbscan_and_visualize(X, k=300, eps=15, min_samples=3)\n",
    "print(\"Number of clusters extracted:\", len(Clusters))\n",
    "num_clusters = len(Clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from fcmeans import FCM\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import copy\n",
    "import warnings\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from imblearn.over_sampling.base import BaseOverSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.exceptions import raise_isinstance_error\n",
    "from imblearn.utils import check_neighbors_object\n",
    "from imblearn.utils.deprecation import deprecate_parameter\n",
    "import seaborn as sns\n",
    "\n",
    "class FCMCENTERSMOTE(BaseOverSampler):\n",
    "\n",
    "    def __init__(self, sampling_strategy='auto', random_state=None, kmeans_args=None, smote_args=None,\n",
    "                 imbalance_ratio_threshold=1.0, density_power=None, use_minibatch_kmeans=True, n_jobs=1, **kwargs):\n",
    "        super(FCMCENTERSMOTE, self).__init__(sampling_strategy=sampling_strategy, **kwargs)\n",
    "        if kmeans_args is None:\n",
    "            kmeans_args = {}\n",
    "        if smote_args is None:\n",
    "            smote_args = {}\n",
    "        self.imbalance_ratio_threshold = imbalance_ratio_threshold\n",
    "        self.kmeans_args = copy.deepcopy(kmeans_args)\n",
    "        self.smote_args = copy.deepcopy(smote_args)\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "        self.use_minibatch_kmeans = use_minibatch_kmeans\n",
    "        self.density_power = density_power\n",
    "\n",
    "    def _cluster(self, X):\n",
    "        fcm = FCM(**self.kmeans_args)\n",
    "        fcm.fit(X)\n",
    "        fcm_labels = fcm.predict(X)\n",
    "        cluster_assignment = np.asarray(fcm_labels)\n",
    "        # Visualize initial clusters\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=cluster_assignment, palette='deep', legend='full')\n",
    "        plt.title(\"Initial Clusters from FCM\")\n",
    "        # plt.show()\n",
    "        return cluster_assignment\n",
    "\n",
    "    def _filter_clusters(self, X, y, cluster_assignment, minority_class_label):\n",
    "      largest_cluster_label = np.max(np.unique(cluster_assignment))\n",
    "      sparsity_factors = np.zeros((largest_cluster_label + 1,), dtype=np.float64)\n",
    "      minority_mask = (y == minority_class_label)\n",
    "      imbalance_ratio_threshold = self.imbalance_ratio_threshold\n",
    "\n",
    "      if isinstance(imbalance_ratio_threshold, dict):\n",
    "          imbalance_ratio_threshold = imbalance_ratio_threshold.get(minority_class_label, 1.0)\n",
    "\n",
    "      for i in np.unique(cluster_assignment):\n",
    "          cluster = X[cluster_assignment == i]\n",
    "          mask = minority_mask[cluster_assignment == i]\n",
    "          minority_count = np.sum(mask)\n",
    "          majority_count = np.sum(~mask)\n",
    "          imbalance_ratio = (majority_count + 1) / (minority_count + 1)\n",
    "\n",
    "          if (imbalance_ratio < imbalance_ratio_threshold) and (minority_count > 1):\n",
    "              distances = euclidean_distances(cluster[mask])\n",
    "              non_diagonal_distances = distances[~np.eye(distances.shape[0], dtype=bool)]\n",
    "              average_minority_distance = np.mean(non_diagonal_distances) if non_diagonal_distances.size > 0 else 0.0\n",
    "\n",
    "              if average_minority_distance == 0:\n",
    "                  average_minority_distance = 1e-1\n",
    "\n",
    "              density_factor = minority_count / (average_minority_distance ** self.density_power)\n",
    "              sparsity_factors[i] = 1 / density_factor\n",
    "\n",
    "      sparsity_sum = np.sum(sparsity_factors)\n",
    "      if sparsity_sum == 0:\n",
    "          sparsity_sum = 1\n",
    "\n",
    "      sampling_weights = sparsity_factors / sparsity_sum if sparsity_sum != 0 else np.full(sparsity_factors.shape, 1.0)\n",
    "      return sampling_weights\n",
    "    @staticmethod\n",
    "    def smote_oversample_with_cluster_center(X, y, cluster_center, sampling_ratio=1.0, smote_args=None, k=5):\n",
    "        if smote_args is not None and 'k_neighbors' in smote_args:\n",
    "            k = smote_args['k_neighbors']\n",
    "        minority_class = np.unique(y)[np.argmin(np.bincount(y))]\n",
    "        minority_indices = np.where(y == minority_class)[0]\n",
    "\n",
    "        # Ensure the cluster_center belongs to the minority class region\n",
    "        if cluster_center is not None:\n",
    "            num_minority_samples = len(minority_indices)\n",
    "            num_majority_samples = int(sampling_ratio * len(y)) - num_minority_samples\n",
    "            k = min(k, num_minority_samples)\n",
    "            # Find the k-nearest neighbors from the minority class around the cluster center\n",
    "            knn = NearestNeighbors(n_neighbors=k)\n",
    "            knn.fit(X[minority_indices])\n",
    "            nn_indices = knn.kneighbors([cluster_center], return_distance=False)[0]\n",
    "\n",
    "            synthetic_samples = []\n",
    "            for i in range(num_minority_samples):\n",
    "                nn_index = np.random.choice(nn_indices)\n",
    "                diff = X[minority_indices[nn_index]] - cluster_center\n",
    "                synthetic_sample = cluster_center + np.random.rand() * diff\n",
    "                synthetic_samples.append(synthetic_sample)\n",
    "            synthetic_samples = np.array(synthetic_samples)\n",
    "\n",
    "\n",
    "            # Resample X and y\n",
    "            X_resampled = np.vstack((X, synthetic_samples))\n",
    "            y_resampled = np.hstack((y, np.full(len(synthetic_samples), minority_class)))\n",
    "\n",
    "            # Shuffle the resampled dataset\n",
    "            shuffle_indices = np.random.permutation(len(X_resampled))\n",
    "            X_resampled = X_resampled[shuffle_indices]\n",
    "            y_resampled = y_resampled[shuffle_indices]\n",
    "\n",
    "            return X_resampled, y_resampled\n",
    "        else:\n",
    "            return X, y\n",
    "\n",
    "\n",
    "    def _fit_resample(self, X, y):\n",
    "        \"\"\"Resample the dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray, shape (n_samples, n_features)\n",
    "            Matrix containing the data which have to be sampled.\n",
    "\n",
    "        y : ndarray, shape (n_samples, )\n",
    "            Corresponding label for each sample in X.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_resampled : ndarray, shape (n_samples_new, n_features)\n",
    "            The array containing the resampled data.\n",
    "\n",
    "        y_resampled : ndarray, shape (n_samples_new)\n",
    "            The corresponding labels of ``X_resampled``\n",
    "\n",
    "        \"\"\"\n",
    "        self._set_subalgorithm_params()\n",
    "\n",
    "        if self.density_power is None:\n",
    "            self.density_power = X.shape[1]\n",
    "\n",
    "        resampled = [ (X.copy(), y.copy()) ]\n",
    "        sampling_ratio = {k: v for k, v in self.sampling_strategy_.items()}\n",
    "        # sampling_strategy_ does not contain classes where n_samples 0\n",
    "        for class_label in np.unique(y):\n",
    "            if class_label not in sampling_ratio:\n",
    "                sampling_ratio[class_label] = 0\n",
    "        for minority_class_label, n_samples in sampling_ratio.items():\n",
    "            if n_samples == 0:\n",
    "                continue\n",
    "\n",
    "            cluster_assignment = self._cluster(X)\n",
    "            sampling_weights = self._filter_clusters(X, y, cluster_assignment, minority_class_label)\n",
    "            smote_args = self.smote_args.copy()\n",
    "            if np.count_nonzero(sampling_weights) > 0:\n",
    "                # perform k-means smote\n",
    "                for i in np.unique(cluster_assignment):\n",
    "                    cluster_X = X[cluster_assignment == i]\n",
    "                    cluster_y = y[cluster_assignment == i]\n",
    "                    if sampling_weights[i] > 0:\n",
    "                        # determine ratio for oversampling the current cluster\n",
    "                        target_ratio = {label: np.count_nonzero(cluster_y == label) for label in sampling_ratio}\n",
    "                        cluster_minority_count = np.count_nonzero(cluster_y == minority_class_label)\n",
    "                        generate_count = int(round(n_samples * sampling_weights[i]))\n",
    "                        target_ratio[minority_class_label] = generate_count + cluster_minority_count\n",
    "\n",
    "                        # make sure that cluster_y has more than 1 class, adding a random point otherwise\n",
    "                        remove_index = -1\n",
    "                        if np.unique(cluster_y).size < 2:\n",
    "                            remove_index = cluster_y.size\n",
    "                            cluster_X = np.append(cluster_X, np.zeros((1,cluster_X.shape[1])), axis=0)\n",
    "                            majority_class_label = next( key for key in sampling_ratio.keys() if key != minority_class_label )\n",
    "                            target_ratio[majority_class_label] = 1 + target_ratio[majority_class_label]\n",
    "                            cluster_y = np.append(cluster_y, np.asarray(majority_class_label).reshape((1,)), axis=0)\n",
    "\n",
    "                        # clear target ratio of labels not present in cluster\n",
    "                        for label in list(target_ratio.keys()):\n",
    "                            if label not in cluster_y:\n",
    "                                del target_ratio[label]\n",
    "\n",
    "                        # # modify copy of the user defined smote_args to reflect computed parameters\n",
    "                        # smote_args['sampling_strategy'] = target_ratio\n",
    "\n",
    "                        # smote_args = self._validate_smote_args(smote_args, cluster_minority_count)\n",
    "                        # # Get the center of the cluster to use as the point for SMOTE oversampling\n",
    "                        cluster_center = np.mean(cluster_X, axis=0)\n",
    "                        # k_value = smote_args['k_neighbors']\n",
    "                        # X_resampled_cluster, y_resampled_cluster = self.smote_oversample_with_cluster_center(\n",
    "                        #     X, y, cluster_center, sampling_ratio=n_samples / X.shape[0],\n",
    "                        #     k=k_value)\n",
    "\n",
    "                        # if k_neighbors is 0, perform random oversampling instead of smote\n",
    "                        if 'k_neighbors' in smote_args and smote_args['k_neighbors'] == 0:\n",
    "                                oversampler_args = {}\n",
    "                                if 'random_state' in smote_args:\n",
    "                                    oversampler_args['random_state'] = smote_args['random_state']\n",
    "                                oversampler = RandomOverSampler(**oversampler_args)\n",
    "\n",
    "                        # finally, apply smote to cluster\n",
    "                        with warnings.catch_warnings():\n",
    "                            # ignore warnings about minority class getting bigger than majority class\n",
    "                            # since this would only be true within this cluster\n",
    "                            warnings.filterwarnings(action='ignore', category=UserWarning, message=r'After over-sampling, the number of samples \\(.*\\) in class .* will be larger than the number of samples in the majority class \\(class #.* \\-\\> .*\\)')\n",
    "                            cluster_resampled_X, cluster_resampled_y = self.smote_oversample_with_cluster_center(\n",
    "                            cluster_X, cluster_y, cluster_center, sampling_ratio=n_samples / X.shape[0],\n",
    "                            k=smote_args['k_neighbors'])\n",
    "\n",
    "                        if remove_index > -1:\n",
    "                            # since SMOTE's results are ordered the same way as the data passed into it,\n",
    "                            # the temporarily added point is at the same index position as it was added.\n",
    "                            for l in [cluster_resampled_X, cluster_resampled_y, cluster_X, cluster_y]:\n",
    "                                np.delete(l, remove_index, 0)\n",
    "\n",
    "                        # add new generated samples to resampled\n",
    "                        resampled.append( (\n",
    "                            cluster_resampled_X[cluster_y.size:,:],\n",
    "                            cluster_resampled_y[cluster_y.size:]))\n",
    "            else:\n",
    "                # all weights are zero -> perform regular smote\n",
    "                warnings.warn('No minority clusters found for class {}. Performing regular SMOTE. Try changing the number of clusters.'.format(minority_class_label))\n",
    "                target_ratio = {label: np.count_nonzero(y == label) for label in sampling_ratio}\n",
    "                target_ratio[minority_class_label] = sampling_ratio[minority_class_label]\n",
    "                minority_count = np.count_nonzero(y == minority_class_label)\n",
    "                smote_args = self._validate_smote_args(smote_args, minority_count)\n",
    "                # Get the center of the cluster to use as the point for SMOTE oversampling\n",
    "                cluster_center = np.mean(X, axis=0)\n",
    "                X_resampled_cluster, y_resampled_cluster = self.smote_oversample_with_cluster_center(\n",
    "                    X, y, cluster_center, sampling_ratio=n_samples / X.shape[0],\n",
    "                            k=smote_args['k_neighbors'])\n",
    "\n",
    "\n",
    "        resampled = list(zip(*resampled))\n",
    "        if(len(resampled) > 0):\n",
    "            X_resampled = np.concatenate(resampled[0], axis=0)\n",
    "            y_resampled = np.concatenate(resampled[1], axis=0)\n",
    "        return X_resampled, y_resampled\n",
    "\n",
    "    def _validate_smote_args(self, smote_args, minority_count):\n",
    "      max_k_neighbors = minority_count - 1\n",
    "      if 'k' in smote_args and smote_args['k'] > max_k_neighbors:\n",
    "          smote_args['k'] = max_k_neighbors\n",
    "      return smote_args\n",
    "\n",
    "    def _set_subalgorithm_params(self):\n",
    "      if self.random_state is not None:\n",
    "          if 'random_state' not in self.smote_args:\n",
    "              self.smote_args['random_state'] = self.random_state\n",
    "          if 'random_state' not in self.kmeans_args:\n",
    "              self.kmeans_args['random_state'] = self.random_state\n",
    "\n",
    "      if self.n_jobs is not None:\n",
    "          if 'n_jobs' not in self.smote_args:\n",
    "              self.smote_args['n_jobs'] = self.n_jobs\n",
    "          if 'n_jobs' not in self.kmeans_args:\n",
    "              if not self.use_minibatch_kmeans:\n",
    "                  self.kmeans_args['n_jobs'] = self.n_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calcul des instances par classe\n",
    "class_counts = dict(zip(*np.unique(y, return_counts=True)))\n",
    "\n",
    "# Affichage du nombre d'instances par classe\n",
    "for label, count in class_counts.items():\n",
    "    print('Class {} has {} instances'.format(label, count))\n",
    "\n",
    "# Création et utilisation de FCM_smote\n",
    "FCM_smote = FCMCENTERSMOTE(\n",
    "    kmeans_args={'n_clusters': num_clusters},\n",
    "    smote_args={'k_neighbors': 5},\n",
    "    imbalance_ratio_threshold=1,\n",
    "    density_power=4\n",
    ")\n",
    "X_resampled, y_resampled = FCM_smote.fit_resample(X.values, y.values)\n",
    "\n",
    "[print('Class {} has {} instances after oversampling'.format(label, count))\n",
    " for label, count in zip(*np.unique(y_resampled, return_counts=True))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from umap import UMAP\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply standard scaling (if necessary)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Scale original dataset features\n",
    "X_resampled_scaled = scaler.transform(X_resampled)  # Scale resampled dataset\n",
    "\n",
    "# Step 1: Fit UMAP on the resampled dataset\n",
    "umap = UMAP(n_components=2, init='random', random_state=42)\n",
    "proj_resampled = umap.fit_transform(X_resampled_scaled)\n",
    "\n",
    "# Step 2: Fit UMAP on the original dataset for comparison\n",
    "proj_original = umap.transform(X_scaled)\n",
    "\n",
    "# Step 3: Create a DataFrame to store UMAP projections for resampled data\n",
    "df_resampled = pd.DataFrame(proj_resampled, columns=['UMAP1', 'UMAP2'])\n",
    "df_resampled['Class'] = y_resampled  # Add class labels for resampled data\n",
    "\n",
    "# Step 4: Identify synthetic points (newly added) by checking for duplicates\n",
    "original_length = len(X)  # Number of instances before resampling\n",
    "df_resampled['is_synthetic'] = ['Synthetic' if i >= original_length else 'Original' for i in range(len(X_resampled))]\n",
    "\n",
    "# Step 5: Plot the result using Plotly\n",
    "fig = px.scatter(\n",
    "    df_resampled, x='UMAP1', y='UMAP2', \n",
    "    color='Class', symbol='is_synthetic', \n",
    "    title=\"UMAP Projection of Resampled Data\",\n",
    "    labels={'color': 'Class'}\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"Shape of X_resampled:\", X_resampled.shape)\n",
    "print(\"Shape of y_resampled:\", y_resampled.shape)\n",
    "print(\"Data type of X_resampled:\", X_resampled.dtype)\n",
    "print(\"Data type of y_resampled:\", y_resampled.dtype)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize UMAP and reduce dimensions\n",
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "X_embedded = reducer.fit_transform(X_resampled)\n",
    "\n",
    "# Scatter plot of the resampled data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y_resampled, cmap='Spectral', s=10)\n",
    "plt.colorbar(boundaries=np.arange(len(np.unique(y_resampled))+1)-0.5).set_ticks(np.arange(len(np.unique(y_resampled))))\n",
    "plt.title('2D UMAP projection of resampled data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import recall_score, accuracy_score, confusion_matrix, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "print(\"Resampled data\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=42)\n",
    "\n",
    "# Define a function to compute specificity for multi-class\n",
    "def specificity(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    specificities = []\n",
    "    for i in range(len(cm)):  # Loop through each class\n",
    "        tn = sum(sum(cm)) - (sum(cm[i, :]) + sum(cm[:, i]) - cm[i, i])\n",
    "        fp = sum(cm[:, i]) - cm[i, i]\n",
    "        specificity_class = tn / (tn + fp)\n",
    "        specificities.append(specificity_class)\n",
    "    return np.mean(specificities)\n",
    "\n",
    "# Sensitivity (same as recall, macro average)\n",
    "def sensitivity(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# Initialize and train the k-NN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics for multi-class classification\n",
    "recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "specificity_val = specificity(y_test, y_pred)\n",
    "g_mean = (recall * specificity_val) ** 0.5\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"Recall (macro):\", recall)\n",
    "print(\"Specificity (macro):\", specificity_val)\n",
    "print(\"Precision (macro):\", metrics.precision_score(y_test, y_pred, average=\"macro\"))\n",
    "print(\"Recall (macro):\", metrics.recall_score(y_test, y_pred, average=\"macro\"))\n",
    "print(\"G-Mean:\", g_mean)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1-score (macro):\", metrics.f1_score(y_test, y_pred, average=\"macro\"))\n",
    "\n",
    "# For AUC in multi-class, use the OneVsRestClassifier approach\n",
    "try:\n",
    "    print(\"AUC:\", roc_auc_score(y_test, y_pred, multi_class='ovr', average='macro'))\n",
    "except ValueError as e:\n",
    "    print(\"AUC could not be calculated:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Plot original data\n",
    "if isinstance(X, pd.DataFrame):\n",
    "    X = X.to_numpy()\n",
    "if isinstance(y, pd.Series):\n",
    "    y = y.to_numpy()\n",
    "\n",
    "# Display the count of original data points\n",
    "print(f\"Original data point count: {X.shape[0]}\")\n",
    "\n",
    "# Selecting two features (feature 0 and feature 1) for the graph, assuming 2D or more data\n",
    "if X.shape[1] >= 2:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Add jitter to separate overlapping points slightly\n",
    "    jittered_X = X + np.random.normal(scale=0.01, size=X.shape)\n",
    "    # Create scatter plot with different colors for different classes\n",
    "    sns.scatterplot(x=jittered_X[:, 0], y=jittered_X[:, 1], hue=y, palette='Set1', s=60, edgecolor='k')\n",
    "    plt.title(f\"Original Data (Total points: {X.shape[0]})\")\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend(title='Class')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient features for 2D plot.\")\n",
    "\n",
    "# Plot resampled data\n",
    "if isinstance(X_resampled, pd.DataFrame):\n",
    "    X_resampled = X_resampled.to_numpy()\n",
    "if isinstance(y_resampled, pd.Series):\n",
    "    y_resampled = y_resampled.to_numpy()\n",
    "\n",
    "# Display the count of resampled data points\n",
    "print(f\"Resampled data point count: {X_resampled.shape[0]}\")\n",
    "\n",
    "# Selecting two features (feature 0 and feature 1) for the graph, assuming 2D or more data\n",
    "if X_resampled.shape[1] >= 2:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Add jitter to separate overlapping points slightly\n",
    "    \n",
    "    jittered_X_resampled = X_resampled + np.random.normal(scale=0.01, size=X_resampled.shape)\n",
    "    # Create scatter plot with different colors for different classes\n",
    "    sns.scatterplot(x=jittered_X_resampled[:, 0], y=jittered_X_resampled[:, 1], hue=y_resampled, palette='Set2', s=60, edgecolor='k', marker='D')\n",
    "    plt.title(f\"Resampled Data (Total points: {X_resampled.shape[0]})\")\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend(title='Class')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Insufficient features for 2D plot.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
