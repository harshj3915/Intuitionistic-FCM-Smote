{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshj3915/Intuitionistic-FCM-Smote/blob/Meet/Fuzzy_C_Means_CENTER_SMOTE_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fuzzy-c-means"
      ],
      "metadata": {
        "id": "YKA3qnuqncDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q numpy scikit-learn imbalanced-learn"
      ],
      "metadata": {
        "id": "--CcGdww1OMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDrjyZ0ffUZz"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_k_clusters_with_dbscan(X, k):\n",
        "    \"\"\"\n",
        "    Extrait les k premiers clusters à partir des données en utilisant l'algorithme DBSCAN.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    Args:\n",
        "    - X : Matrice des features\n",
        "    - k : Nombre de clusters à extraire\n",
        "\n",
        "    Returns:\n",
        "    - clusters : Liste des k premiers clusters\n",
        "    \"\"\"\n",
        "    # Appliquer DBSCAN pour obtenir les clusters\n",
        "    dbscan = DBSCAN()\n",
        "    dbscan.fit(X)\n",
        "\n",
        "    # Obtenir les labels des clusters\n",
        "    cluster_labels = dbscan.labels_\n",
        "\n",
        "    # Identifier les k premiers clusters\n",
        "    unique_labels = np.unique(cluster_labels)\n",
        "    k_clusters = []\n",
        "    for label in unique_labels:\n",
        "        if label != -1 and len(k_clusters) < k:  # Ignorer le bruit (-1) et obtenir k clusters\n",
        "            cluster = X[cluster_labels == label]\n",
        "            k_clusters.append(cluster)\n",
        "\n",
        "\n",
        "\n",
        "    return k_clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import math\n",
        "import copy\n",
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.utils import check_random_state\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from imblearn.over_sampling.base import BaseOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.exceptions import raise_isinstance_error\n",
        "from imblearn.utils import check_neighbors_object\n",
        "from imblearn.utils.deprecation import deprecate_parameter\n",
        "\n"
      ],
      "metadata": {
        "id": "-0KyNSmAgK0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U imbalanced-learn"
      ],
      "metadata": {
        "id": "RlvokjBWGoBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from fcmeans import FCM\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "#from imblearn.over_sampling import BaseOverSampler, SMOTE\n",
        "import copy\n",
        "\n",
        "class FCMCENTERSMOTE(BaseOverSampler):\n",
        "\n",
        "    def __init__(self, sampling_strategy='auto', random_state=None, kmeans_args=None, smote_args=None,\n",
        "                 imbalance_ratio_threshold=1.0, density_power=None, use_minibatch_kmeans=True, n_jobs=1, **kwargs):\n",
        "        super(FCMCENTERSMOTE, self).__init__(sampling_strategy=sampling_strategy, **kwargs)\n",
        "        if kmeans_args is None:\n",
        "            kmeans_args = {}\n",
        "        if smote_args is None:\n",
        "            smote_args = {}\n",
        "        self.imbalance_ratio_threshold = imbalance_ratio_threshold\n",
        "        self.kmeans_args = copy.deepcopy(kmeans_args)\n",
        "        self.smote_args = copy.deepcopy(smote_args)\n",
        "        self.random_state = random_state\n",
        "        self.n_jobs = n_jobs\n",
        "        self.use_minibatch_kmeans = use_minibatch_kmeans\n",
        "        self.density_power = density_power\n",
        "\n",
        "    def _cluster(self, X):\n",
        "        fcm = FCM(**self.kmeans_args)\n",
        "        fcm.fit(X)\n",
        "        fcm_labels = fcm.predict(X)\n",
        "        cluster_assignment = np.asarray(fcm_labels)\n",
        "        return cluster_assignment\n",
        "\n",
        "    def _filter_clusters(self, X, y, cluster_assignment, minority_class_label):\n",
        "      largest_cluster_label = np.max(np.unique(cluster_assignment))\n",
        "      sparsity_factors = np.zeros((largest_cluster_label + 1,), dtype=np.float64)\n",
        "      minority_mask = (y == minority_class_label)\n",
        "      imbalance_ratio_threshold = self.imbalance_ratio_threshold\n",
        "\n",
        "      if isinstance(imbalance_ratio_threshold, dict):\n",
        "          imbalance_ratio_threshold = imbalance_ratio_threshold.get(minority_class_label, 1.0)\n",
        "\n",
        "      for i in np.unique(cluster_assignment):\n",
        "          cluster = X[cluster_assignment == i]\n",
        "          mask = minority_mask[cluster_assignment == i]\n",
        "          minority_count = np.sum(mask)\n",
        "          majority_count = np.sum(~mask)\n",
        "          imbalance_ratio = (majority_count + 1) / (minority_count + 1)\n",
        "\n",
        "          if (imbalance_ratio < imbalance_ratio_threshold) and (minority_count > 1):\n",
        "              distances = euclidean_distances(cluster[mask])\n",
        "              non_diagonal_distances = distances[~np.eye(distances.shape[0], dtype=bool)]\n",
        "              average_minority_distance = np.mean(non_diagonal_distances) if non_diagonal_distances.size > 0 else 0.0\n",
        "\n",
        "              if average_minority_distance == 0:\n",
        "                  average_minority_distance = 1e-1\n",
        "\n",
        "              density_factor = minority_count / (average_minority_distance ** self.density_power)\n",
        "              sparsity_factors[i] = 1 / density_factor\n",
        "\n",
        "      sparsity_sum = np.sum(sparsity_factors)\n",
        "      if sparsity_sum == 0:\n",
        "          sparsity_sum = 1\n",
        "\n",
        "      sampling_weights = sparsity_factors / sparsity_sum if sparsity_sum != 0 else np.full(sparsity_factors.shape, 1.0)\n",
        "      return sampling_weights\n",
        "    @staticmethod\n",
        "    def smote_oversample_with_point_value(X, y, point_index, sampling_ratio=1.0,smote_args= None,k=5):\n",
        "      if smote_args is not None and 'k_neighbors' in smote_args:\n",
        "            k = smote_args['k_neighbors']\n",
        "      minority_class = np.unique(y)[np.argmin(np.bincount(y))]\n",
        "      minority_indices = np.where(y == minority_class)[0]\n",
        "\n",
        "      if isinstance(point_index, int) and point_index < len(y) and y[point_index] == minority_class:\n",
        "        num_minority_samples = len(minority_indices)\n",
        "        num_majority_samples = int(sampling_ratio * len(y)) - num_minority_samples\n",
        "\n",
        "        knn = NearestNeighbors(n_neighbors=k + 1)\n",
        "        knn.fit(X[minority_indices])\n",
        "        nn_indices = knn.kneighbors([X[point_index]], return_distance=False)[0][1:]\n",
        "\n",
        "        synthetic_samples = []\n",
        "        for i in range(num_minority_samples):\n",
        "            nn_index = np.random.choice(nn_indices)\n",
        "            diff = X[nn_index] - X[point_index]\n",
        "            synthetic_sample = X[point_index] + np.random.rand() * diff\n",
        "            synthetic_samples.append(synthetic_sample)\n",
        "        synthetic_samples = np.array(synthetic_samples)\n",
        "\n",
        "        X_resampled = np.vstack((X, synthetic_samples))\n",
        "        y_resampled = np.hstack((y, np.full(len(synthetic_samples), minority_class)))\n",
        "\n",
        "        shuffle_indices = np.random.permutation(len(X_resampled))\n",
        "        X_resampled = X_resampled[shuffle_indices]\n",
        "        y_resampled = y_resampled[shuffle_indices]\n",
        "\n",
        "        return X_resampled, y_resampled\n",
        "      else:\n",
        "        return X, y\n",
        "\n",
        "    def _fit_resample(self, X, y):\n",
        "        \"\"\"Resample the dataset.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : ndarray, shape (n_samples, n_features)\n",
        "            Matrix containing the data which have to be sampled.\n",
        "\n",
        "        y : ndarray, shape (n_samples, )\n",
        "            Corresponding label for each sample in X.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        X_resampled : ndarray, shape (n_samples_new, n_features)\n",
        "            The array containing the resampled data.\n",
        "\n",
        "        y_resampled : ndarray, shape (n_samples_new)\n",
        "            The corresponding labels of ``X_resampled``\n",
        "\n",
        "        \"\"\"\n",
        "        self._set_subalgorithm_params()\n",
        "\n",
        "        if self.density_power is None:\n",
        "            self.density_power = X.shape[1]\n",
        "\n",
        "        resampled = [ (X.copy(), y.copy()) ]\n",
        "        sampling_ratio = {k: v for k, v in self.sampling_strategy_.items()}\n",
        "        # sampling_strategy_ does not contain classes where n_samples 0\n",
        "        for class_label in np.unique(y):\n",
        "            if class_label not in sampling_ratio:\n",
        "                sampling_ratio[class_label] = 0\n",
        "        for minority_class_label, n_samples in sampling_ratio.items():\n",
        "            if n_samples == 0:\n",
        "                continue\n",
        "\n",
        "            cluster_assignment = self._cluster(X)\n",
        "            sampling_weights = self._filter_clusters(X, y, cluster_assignment, minority_class_label)\n",
        "            smote_args = self.smote_args.copy()\n",
        "            if np.count_nonzero(sampling_weights) > 0:\n",
        "                # perform k-means smote\n",
        "                for i in np.unique(cluster_assignment):\n",
        "                    cluster_X = X[cluster_assignment == i]\n",
        "                    cluster_y = y[cluster_assignment == i]\n",
        "                    if sampling_weights[i] > 0:\n",
        "                        # determine ratio for oversampling the current cluster\n",
        "                        target_ratio = {label: np.count_nonzero(cluster_y == label) for label in sampling_ratio}\n",
        "                        cluster_minority_count = np.count_nonzero(cluster_y == minority_class_label)\n",
        "                        generate_count = int(round(n_samples * sampling_weights[i]))\n",
        "                        target_ratio[minority_class_label] = generate_count + cluster_minority_count\n",
        "\n",
        "                        # make sure that cluster_y has more than 1 class, adding a random point otherwise\n",
        "                        remove_index = -1\n",
        "                        if np.unique(cluster_y).size < 2:\n",
        "                            remove_index = cluster_y.size\n",
        "                            cluster_X = np.append(cluster_X, np.zeros((1,cluster_X.shape[1])), axis=0)\n",
        "                            majority_class_label = next( key for key in sampling_ratio.keys() if key != minority_class_label )\n",
        "                            target_ratio[majority_class_label] = 1 + target_ratio[majority_class_label]\n",
        "                            #cluster_y = np.append(cluster_y, np.asarray(majority_class_label).reshape((1,)), axis=0)\n",
        "\n",
        "                        # clear target ratio of labels not present in cluster\n",
        "                        for label in list(target_ratio.keys()):\n",
        "                            if label not in cluster_y:\n",
        "                                del target_ratio[label]\n",
        "\n",
        "                        # modify copy of the user defined smote_args to reflect computed parameters\n",
        "                        smote_args['sampling_strategy'] = target_ratio\n",
        "\n",
        "                        smote_args = self._validate_smote_args(smote_args, cluster_minority_count)\n",
        "                        # Get the center of the cluster to use as the point for SMOTE oversampling\n",
        "                        cluster_center = np.mean(cluster_X, axis=0)\n",
        "                        k_value = smote_args['k_neighbors']\n",
        "                        X_resampled_cluster, y_resampled_cluster = self.smote_oversample_with_point_value(\n",
        "                            X, y, cluster_center, sampling_ratio=n_samples / X.shape[0],\n",
        "                            k=k_value)\n",
        "\n",
        "                        # if k_neighbors is 0, perform random oversampling instead of smote\n",
        "                        if 'k_neighbors' in smote_args and smote_args['k_neighbors'] == 0:\n",
        "                                oversampler_args = {}\n",
        "                                if 'random_state' in smote_args:\n",
        "                                    oversampler_args['random_state'] = smote_args['random_state']\n",
        "                                oversampler = RandomOverSampler(**oversampler_args)\n",
        "\n",
        "                        # finally, apply smote to cluster\n",
        "                        with warnings.catch_warnings():\n",
        "                            # ignore warnings about minority class getting bigger than majority class\n",
        "                            # since this would only be true within this cluster\n",
        "                            warnings.filterwarnings(action='ignore', category=UserWarning, message=r'After over-sampling, the number of samples \\(.*\\) in class .* will be larger than the number of samples in the majority class \\(class #.* \\-\\> .*\\)')\n",
        "                            cluster_resampled_X, cluster_resampled_y = self.smote_oversample_with_point_value(\n",
        "                            X, y, cluster_center, sampling_ratio=n_samples / X.shape[0],\n",
        "                            k=smote_args['k_neighbors'])\n",
        "\n",
        "                        if remove_index > -1:\n",
        "                            # since SMOTE's results are ordered the same way as the data passed into it,\n",
        "                            # the temporarily added point is at the same index position as it was added.\n",
        "                            for l in [cluster_resampled_X, cluster_resampled_y, cluster_X, cluster_y]:\n",
        "                                np.delete(l, remove_index, 0)\n",
        "\n",
        "                        # add new generated samples to resampled\n",
        "                        resampled.append( (\n",
        "                            cluster_resampled_X[cluster_y.size:,:],\n",
        "                            cluster_resampled_y[cluster_y.size:]))\n",
        "            else:\n",
        "                # all weights are zero -> perform regular smote\n",
        "                warnings.warn('No minority clusters found for class {}. Performing regular SMOTE. Try changing the number of clusters.'.format(minority_class_label))\n",
        "                target_ratio = {label: np.count_nonzero(y == label) for label in sampling_ratio}\n",
        "                target_ratio[minority_class_label] = sampling_ratio[minority_class_label]\n",
        "                minority_count = np.count_nonzero(y == minority_class_label)\n",
        "                smote_args = self._validate_smote_args(smote_args, minority_count)\n",
        "                # Get the center of the cluster to use as the point for SMOTE oversampling\n",
        "                cluster_center = np.mean(cluster_X, axis=0)\n",
        "                X_resampled_cluster, y_resampled_cluster = self.smote_oversample_with_point_value(\n",
        "                    X, y, cluster_center, sampling_ratio=n_samples / X.shape[0],\n",
        "                            k=smote_args['k_neighbors'])\n",
        "\n",
        "\n",
        "        resampled = list(zip(*resampled))\n",
        "        if(len(resampled) > 0):\n",
        "            X_resampled = np.concatenate(resampled[0], axis=0)\n",
        "            y_resampled = np.concatenate(resampled[1], axis=0)\n",
        "        return X_resampled, y_resampled\n",
        "\n",
        "    def _validate_smote_args(self, smote_args, minority_count):\n",
        "      max_k_neighbors = minority_count - 1\n",
        "      if 'k' in smote_args and smote_args['k'] > max_k_neighbors:\n",
        "          smote_args['k'] = max_k_neighbors\n",
        "      return smote_args\n",
        "\n",
        "    def _set_subalgorithm_params(self):\n",
        "      if self.random_state is not None:\n",
        "          if 'random_state' not in self.smote_args:\n",
        "              self.smote_args['random_state'] = self.random_state\n",
        "          if 'random_state' not in self.kmeans_args:\n",
        "              self.kmeans_args['random_state'] = self.random_state\n",
        "\n",
        "      if self.n_jobs is not None:\n",
        "          if 'n_jobs' not in self.smote_args:\n",
        "              self.smote_args['n_jobs'] = self.n_jobs\n",
        "          if 'n_jobs' not in self.kmeans_args:\n",
        "              if not self.use_minibatch_kmeans:\n",
        "                  self.kmeans_args['n_jobs'] = self.n_jobs\n"
      ],
      "metadata": {
        "id": "iguS7GUPjN0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/vehicle_csv.csv', sep=\",\")\n",
        "#header=None\n",
        "#df[0], df[22] = df[22].copy(), df[0].copy()\n",
        "#df[0]=y\n",
        "#df[22] = df[22].replace(0,-1)\n",
        "#df['Rings'] = df['Rings'].replace(16, -1)\n",
        "#df['Rings'] = df['Rings'].replace(6, 1)\n",
        "#df[0] = df[0].replace(2, 1)\n",
        "#df['test'] = df['test'].replace('Iris-versicolor', 0)\n",
        "#df['test'] = df['test'].replace(' pp', 0)\n",
        "#y=df['test'].to_numpy()\n",
        "df['Class'] = df['Class'].replace('van', -1)\n",
        "df['Class'] = df['Class'].replace('saab', 1)\n",
        "df['Class'] = df['Class'].replace('bus', 1)\n",
        "df['Class'] = df['Class'].replace('opel', 1)\n",
        "X = df.iloc[:,0:18].to_numpy()\n",
        "y=df.iloc[:,18].to_numpy()\n",
        "#df[0], df[22] = df[22].copy(), df[0].copy()\n",
        "\n",
        "# Afficher le DataFrame après l'échange\n",
        "#df.to_csv('/content/SPECT.csv', index=False)\n",
        "#df['Outcome']\n",
        "nombre_de_moins_un = (y == 1).sum()\n",
        "\n",
        "# Afficher le nombre d'occurrences\n",
        "print(\"Nombre d'occurrences de -1 dans la colonne 13:\", nombre_de_moins_un)\n",
        "#df_selection = df.loc[df.iloc[:, -1].isin([-1, 1])]\n",
        "# Afficher le DataFrame après la sélection\n",
        "#df_selection['Sex'] = df_selection['Sex'].replace('M',1)\n",
        "#df_selection['Sex'] = df_selection['Sex'].replace('F',2)\n",
        "#df_selection['Sex'] = df_selection['Sex'].replace('I',0)\n",
        "\n",
        "df.to_csv('vehicle.csv',index=False)\n",
        "six_classes = df['Class'].unique()\n",
        "\n",
        "# Afficher les six classes distinctes\n",
        "print(six_classes)\n"
      ],
      "metadata": {
        "id": "1gPEG1QSzR3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6300630b-272d-49bb-9bd9-3437a6084380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre d'occurrences de -1 dans la colonne 13: 647\n",
            "[-1  1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calcul des instances par classe\n",
        "class_counts = dict(zip(*np.unique(y, return_counts=True)))\n",
        "\n",
        "# Affichage du nombre d'instances par classe\n",
        "for label, count in class_counts.items():\n",
        "    print('Class {} has {} instances'.format(label, count))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCP0hoLm2z3q",
        "outputId": "4fd1fa49-6048-4607-a646-f61ede33ea70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0 has 16649 instances\n",
            "Class 1 has 1827 instances\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapping2 = dict(mapping.values)"
      ],
      "metadata": {
        "id": "p3d3RP754-F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'X' contains your data\n",
        "\n",
        "# Instantiating DBSCAN\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=4)  # You may need to adjust eps and min_samples\n",
        "\n",
        "# Fitting DBSCAN to your data\n",
        "clusters = dbscan.fit_predict(X)\n",
        "\n",
        "# Getting unique cluster labels (excluding noise, labeled as -1)\n",
        "unique_labels = np.unique(clusters)\n",
        "num_clusters = len(unique_labels[unique_labels != -1])\n",
        "\n",
        "print(f\"Number of clusters found by DBSCAN: {num_clusters}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o77BUYyE4dX-",
        "outputId": "93de30ce-d90d-492a-b616-1350b2921c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of clusters found by DBSCAN: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Calcul des instances par classe\n",
        "class_counts = dict(zip(*np.unique(y, return_counts=True)))\n",
        "\n",
        "# Affichage du nombre d'instances par classe\n",
        "for label, count in class_counts.items():\n",
        "    print('Class {} has {} instances'.format(label, count))\n",
        "\n",
        "# Création et utilisation de FCM_smote\n",
        "FCM_smote = FCMCENTERSMOTE(\n",
        "    kmeans_args={'n_clusters': num_clusters},\n",
        "    smote_args={'k_neighbors': 5},\n",
        "    imbalance_ratio_threshold=1,\n",
        "    density_power=4\n",
        ")\n",
        "X_resampled, y_resampled = FCM_smote.fit_resample(X, y)\n",
        "\n",
        "[print('Class {} has {} instances after oversampling'.format(label, count))\n",
        " for label, count in zip(*np.unique(y_resampled, return_counts=True))]"
      ],
      "metadata": {
        "id": "FUgYTFVJzRt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "y0A3-DE13_ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "-yVlbLoRsZou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialiser le classificateur k-NN avec k=3\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# Entraîner le modèle\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Prédire les étiquettes sur l'ensemble de test\n",
        "y_pred = knn.predict(X_test)"
      ],
      "metadata": {
        "id": "y1Uk9AaQsePf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "clf = svm.SVC(kernel='linear')\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "TBXjrV8I4DCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U scikit-learn"
      ],
      "metadata": {
        "id": "SfDfhKC4tcsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import recall_score, accuracy_score, confusion_matrix\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X = df.iloc[:,0:4].to_numpy()\n",
        "y=df.iloc[:,4].to_numpy()\n",
        "df['test']=y\n",
        "df['test'] = df['test'].replace('Iris-setosa', 0)\n",
        "df['test'] = df['test'].replace('Iris-virginica', 1)\n",
        "df['test'] = df['test'].replace('Iris-versicolor', 0)\n",
        "#df['test'] = df['test'].replace(' pp', 0)\n",
        "y=df['test'].to_numpy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "\n",
        "# Calcul de la sensibilité (recall)\n",
        "def sensitivity(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return tp / (tp + fn)\n",
        "\n",
        "# Calcul de la spécificité\n",
        "def specificity(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
        "    return tn / (tn + fp)\n",
        "\n",
        "# Initialiser le classificateur k-NN avec k=3\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "# Entraîner le modèle\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Prédire les étiquettes sur l'ensemble de test\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "recall = recall_score(y_test, y_pred)\n",
        "specificity_val = specificity(y_test, y_pred)\n",
        "g_mean = (recall * specificity_val) ** 0.5\n",
        "print(\"recall\",recall)\n",
        "print(\"specificity_val\",specificity_val)\n",
        "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
        "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
        "print(\"G-M\",g_mean)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"F1-score:\",metrics.f1_score(y_test, y_pred))\n",
        "print(\"AUC:\",metrics.roc_auc_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "1I5LxMVQ4GYG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}